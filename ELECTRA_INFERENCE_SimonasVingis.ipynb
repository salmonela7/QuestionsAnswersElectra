{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hparamseval = {\n",
    "    \"model_size\": \"small\",\n",
    "    \"task_names\": [\"squad\"],\n",
    "    \"do_train\": \"False\",\n",
    "    \"do_eval\": \"True\",\n",
    "    \"init_checkpoint\": \"data/models/electra_small/finetuning_models/squad_model_1/\"\n",
    "}\n",
    "\n",
    "with open(\"hparamseval.json\", \"w\") as f:\n",
    "    json.dump(hparamseval, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRED_FILE = 'data/models/electra_small/results/squad_qa/squad_preds.json'\n",
    "INFERENCE_DATA_FILE = 'data/finetuning_data/squad/dev.json'\n",
    "\n",
    "def readInferenceData(data_file):\n",
    "    f = open(data_file)\n",
    "    data = json.load(f)\n",
    "\n",
    "    qas = []\n",
    "    for i in data['data']:\n",
    "        for j in i['paragraphs']:\n",
    "            context = j['context']\n",
    "            for k in j['qas']:\n",
    "                obj = {'id': k['id'], 'question': k['question'], 'context': context}\n",
    "                qas.append(obj)\n",
    "    f.close()\n",
    "    return qas\n",
    "\n",
    "def readPredictions(predictions_file):\n",
    "    f = open(predictions_file)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "def mapPredictions(qas, preds):\n",
    "    mappedQas = []\n",
    "    for qa in qas:\n",
    "        obj = qa\n",
    "        try:\n",
    "            answer = preds[qa['id']]\n",
    "        except:\n",
    "            continue\n",
    "        obj['answer'] = answer\n",
    "        mappedQas.append(obj)\n",
    "    return mappedQas\n",
    "\n",
    "def printAnswers(data, verbose, file_path=''):\n",
    "    if file_path:\n",
    "        with open(file_path, 'w') as result_file:\n",
    "            lines = []\n",
    "            for i, qa in enumerate(data):\n",
    "                question = qa['question']\n",
    "                answer = qa['answer']\n",
    "                context = qa['context']\n",
    "                lines.append(f'{context},{question},{answer}\\n')\n",
    "\n",
    "            result_file.writelines(lines)\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(json.dumps(data, indent=4))\n",
    "    else:\n",
    "        for i, qa in enumerate(data):\n",
    "            question = qa['question']\n",
    "            answer = qa['answer']\n",
    "            context = qa['context']\n",
    "            print(f'({i+1}): \\nContext: {context} \\nQuestion: {question} \\nAnswer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class InferenceDataManager:\n",
    "    version = \"\"\n",
    "    data = []\n",
    "\n",
    "    def __init__(self, paragraphs):\n",
    "        self.version = \"v2.0\"\n",
    "        self.data = [{\"title\": \"inference\", \"paragraphs\": paragraphs}]\n",
    "\n",
    "    def getObject(self):\n",
    "        obj = {\"version\": self.version, \"data\": self.data}\n",
    "        return obj"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#TESTING THE MODEL\n",
    "\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATASET_METADATA_FILE = 'data/models/electra_small/finetuning_tfrecords/squad_tfrecords/squad_dev.metadata'\n",
    "TEST_SET_FILE = 'inference_test/task_2_test_set_questions.txt'\n",
    "RESULT_OUTPUT_FILE = 'inference_test/results/1813003_10-epoch.csv'\n",
    "\n",
    "def generateInferenceFileUserInput():\n",
    "    paragraphs = []\n",
    "    while 1:\n",
    "        context = input(\"Specify the context of the questions: \\n\\n TYPE x to end selection\")\n",
    "        if context == 'x':\n",
    "            break\n",
    "        qas = []\n",
    "        while 1:\n",
    "            question = input(f'Specify the question for context: \\n\\n {context} \\n\\n TYPE x to choose specify another context')\n",
    "            if question == 'x':\n",
    "                break\n",
    "            qa = {\n",
    "                \"question\": question,\n",
    "                \"id\": ''.join(random.choice('0123456789abcdef') for i in range(24)),\n",
    "                \"is_impossible\": \"\",\n",
    "                \"answers\": []\n",
    "            }\n",
    "            qas.append(qa)\n",
    "        paragraph = {\"qas\": qas, \"context\": context}\n",
    "        paragraphs.append(paragraph)\n",
    "\n",
    "    inf_data_file_json = InferenceDataManager(paragraphs).getObject()\n",
    "    return inf_data_file_json\n",
    "\n",
    "def generateInferenceFileFromTestSet():\n",
    "    paragraphs = []\n",
    "    with open(TEST_SET_FILE, 'r') as file:\n",
    "        for line in file:\n",
    "            context, question = line.split(' ,')\n",
    "            qas = []\n",
    "            qas.append({\n",
    "                \"question\": question.rstrip('\\n'),\n",
    "                \"id\": ''.join(random.choice('0123456789abcdef') for i in range(24)),\n",
    "                \"is_impossible\": \"\",\n",
    "                \"answers\": []\n",
    "            })\n",
    "            paragraph = {\"qas\": qas, \"context\": context}\n",
    "            paragraphs.append(paragraph)\n",
    "\n",
    "    inf_data_file_json = InferenceDataManager(paragraphs).getObject()\n",
    "    return inf_data_file_json\n",
    "\n",
    "def saveInferenceFile(file_path, json_data):\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(json_data, outfile, indent=4)\n",
    "    if os.path.exists(DATASET_METADATA_FILE):\n",
    "        os.remove(DATASET_METADATA_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_size': 'small', 'task_names': ['squad'], 'do_train': 'False', 'do_eval': 'True', 'init_checkpoint': 'data/models/electra_small/finetuning_models/squad_model_1/'}\n",
      "================================================================================\n",
      "Config: model=electra_small, trial 1/1\n",
      "================================================================================\n",
      "answerable_classifier True\n",
      "answerable_uses_start_logits True\n",
      "answerable_weight 0.5\n",
      "beam_size 20\n",
      "data_dir data\n",
      "debug False\n",
      "do_eval True\n",
      "do_lower_case True\n",
      "do_train False\n",
      "doc_stride 128\n",
      "double_unordered True\n",
      "embedding_size 128\n",
      "eval_batch_size 8\n",
      "gcp_project None\n",
      "init_checkpoint data/models/electra_small/finetuning_models/squad_model_1/\n",
      "iterations_per_loop 1000\n",
      "joint_prediction True\n",
      "keep_all_models True\n",
      "layerwise_lr_decay 0.8\n",
      "learning_rate 0.0001\n",
      "log_examples False\n",
      "max_answer_length 30\n",
      "max_query_length 64\n",
      "max_seq_length 512\n",
      "model_dir data\\models\\electra_small\\finetuning_models\\squad_model\n",
      "model_hparam_overrides {}\n",
      "model_name electra_small\n",
      "model_size small\n",
      "n_best_size 20\n",
      "n_writes_test 5\n",
      "num_tpu_cores 1\n",
      "num_train_epochs 10.0\n",
      "num_trials 1\n",
      "predict_batch_size 8\n",
      "preprocessed_data_dir data\\models\\electra_small\\finetuning_tfrecords\\squad_tfrecords\n",
      "qa_eval_file <built-in method format of str object at 0x000001C7A1967730>\n",
      "qa_na_file <built-in method format of str object at 0x000001C7A1967810>\n",
      "qa_na_threshold -2.75\n",
      "qa_preds_file <built-in method format of str object at 0x000001C7A19677A0>\n",
      "raw_data_dir <built-in method format of str object at 0x000001C7A196D260>\n",
      "results_pkl data\\models\\electra_small\\results\\squad_results.pkl\n",
      "results_txt data\\models\\electra_small\\results\\squad_results.txt\n",
      "save_checkpoints_steps 7000\n",
      "task_names ['squad']\n",
      "test_predictions <built-in method format of str object at 0x000001C7A196D3A0>\n",
      "tpu_job_name None\n",
      "tpu_name None\n",
      "tpu_zone None\n",
      "train_batch_size 8\n",
      "use_tfrecords_if_existing True\n",
      "use_tpu False\n",
      "vocab_file data\\vocab.txt\n",
      "vocab_size 30522\n",
      "warmup_proportion 0.1\n",
      "weight_decay_rate 0.01\n",
      "write_distill_outputs False\n",
      "write_test_outputs False\n",
      "\n",
      "================================================================================\n",
      "Run dev set evaluation: model=electra_small, trial 1/1\n",
      "================================================================================\n",
      "Evaluating squad\n",
      "Loading dataset squad_dev\n",
      "Existing tfrecords not found so creating\n",
      "2 examples created, 0 failures\n",
      "Writing example 0 of 2\n",
      "Building model...\n",
      "Building complete\n",
      "squad: NoAns_exact: 50.00 - NoAns_f1: 50.00 - NoAns_total: 2.00 - best_exact: 100.00 - best_exact_thresh: 0.00 - best_f1: 100.00 - best_f1_thresh: 0.00 - exact: 50.00 - f1: 50.00 - total: 2.00\n",
      "\n",
      "Writing results to data\\models\\electra_small\\results\\squad_results.txt\n",
      "config data\\models\\electra_small\\results\\squad_results.txt\n",
      "(1): \n",
      "Context: The car is in a garage. The motorcycle is on the sidewalk. \n",
      "Question: Where is the car? \n",
      "Answer: in a garage\n",
      "\n",
      "(2): \n",
      "Context: Timmys dad likes apples. Timmy likes pears. Timmys dads name is Tom. \n",
      "Question: What does Tom like? \n",
      "Answer: pears\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = generateInferenceFileUserInput()\n",
    "# data = generateInferenceFileFromTestSet()\n",
    "saveInferenceFile(INFERENCE_DATA_FILE, data)\n",
    "\n",
    "%run -i electra/run_finetuning.py --data-dir data --model-name electra_small --hparams \"hparamseval.json\"\n",
    "\n",
    "data = readInferenceData(INFERENCE_DATA_FILE)\n",
    "preds = readPredictions(PRED_FILE)\n",
    "mappedData = mapPredictions(data, preds)\n",
    "\n",
    "# printAnswers(mappedData, True)\n",
    "printAnswers(mappedData, False)\n",
    "# printAnswers(mappedData, False, RESULT_OUTPUT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}